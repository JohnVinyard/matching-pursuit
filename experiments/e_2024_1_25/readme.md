Finally try iterative decomposition, with no sparsification step

Which works better overall, the original setting, or the literally take an optimizer step
each and every time?

The independent approach obviously gets to use more info for each event, so that may be a no-brainer

How do things sound after 1K iterations?

I think it's going to start being important that the analysis and loss both use the same, non-negative representation